{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProjectModelsV7.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqH8ddEDczL5",
        "colab_type": "code",
        "outputId": "f6742e0d-7e6c-4dab-9386-5bbb4abb49f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"\n",
        "Changes done on May 17th by Sharath\n",
        "a)Loading images - written function to load the images, used tqdm to show the progress, it also allows us to resize the image size required\n",
        "b)Display output image - written code to merge the actual and predicted image, so it can be shown side by side.  this is written as cv.imshow \n",
        "    won't work in collab, instead we need to use cv2_imshow which doesnot allow printing side by side images\n",
        "c)Model History graphs - Not yet tested - added code to show the model history on accuracy and loss\n",
        "changes done on 24th May by sharath\n",
        "d)Excel Reporting - added excel reporting to log the run details on each run\n",
        "Changes done on 24th May by Pratosh\n",
        "e)data_convert - function changed to run faster\n",
        "f)Parameter auto run - added excel to track the assignment of runs, so code picks up next combination to run - sharath 24th May\n",
        "g)added earlystopping on epochs\n",
        "h)added IoU metrics based on Pratosh input to model.compile\n",
        "i)added exit at the end to restart the colab each time running\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nChanges done on May 17th by Sharath\\na)Loading images - written function to load the images, used tqdm to show the progress, it also allows us to resize the image size required\\nb)Display output image - written code to merge the actual and predicted image, so it can be shown side by side.  this is written as cv.imshow \\n    won't work in collab, instead we need to use cv2_imshow which doesnot allow printing side by side images\\nc)Model History graphs - Not yet tested - added code to show the model history on accuracy and loss\\nchanges done on 24th May by sharath\\nd)Excel Reporting - added excel reporting to log the run details on each run\\nChanges done on 24th May by Pratosh\\ne)data_convert - function changed to run faster\\nf)Parameter auto run - added excel to track the assignment of runs, so code picks up next combination to run - sharath 24th May\\ng)added earlystopping on epochs\\nh)added IoU metrics based on Pratosh input to model.compile\\ni)added exit at the end to restart the colab each time running\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5yh6zpM4RN0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#set the path of images and log files\n",
        "path_y = \"/content/drive/My Drive/CapstoneImg/seg/seg_f/\"\n",
        "path_x = \"/content/drive/My Drive/CapstoneImg/rgb/rgb_f/\"\n",
        "logPath=\"/content/drive/My Drive/CapstoneImg/\"\n",
        "runnerName=\"Sharath\"\n",
        "n_classes = 12"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9jCyHkRm6u4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "is2khgupDNDb",
        "colab_type": "code",
        "outputId": "59e64ad9-fa30-4bfe-d77b-b07b68888767",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Add,Input,concatenate,BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras.applications import *\n",
        "from keras.layers.convolutional import Conv2D,Conv2DTranspose,MaxPooling2D\n",
        "from keras.layers.core import Activation\n",
        "import tensorflow as tf\n",
        "from keras.backend import tensorflow_backend as K\n",
        "from keras.utils.layer_utils import count_params\n",
        "from keras.callbacks import EarlyStopping\n",
        "import numpy as np\n",
        "import cv2\n",
        "import glob\n",
        "import os\n",
        "from tqdm import tqdm #for progress bars while running to check the current progess of execution\n",
        "#time operation\n",
        "from datetime import datetime\n",
        "import time\n",
        "#Excel operations\n",
        "import openpyxl\n",
        "from openpyxl import Workbook\n",
        "from os import path\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XumZ8yErwBVQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# datetime object containing current date and time\n",
        "stTime = datetime.now()\n",
        "# dd/mm/YY H:M:S\n",
        "dt_string = stTime.strftime(\"%d/%m/%Y %H:%M:%S\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgcM2v8US4ri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read all parameters - remembers last run and hence it picks the next combination\n",
        "lngFound=0 #setting 0 to indicate the model not assigned\n",
        "lngResetRow=0 #Set this to excel row value if we want a combination parameter to be rerun (overrides the sequence)\n",
        "if (path.exists(logPath+\"ModelParameters.xlsx\")==True): #File exist, open it\n",
        "  wbP = openpyxl.load_workbook(logPath+\"ModelParameters.xlsx\")\n",
        "  wsP = wbP.active\n",
        "  colummn=wsP['A']\n",
        "  lastRow=len(colummn)+1\n",
        "  lastRun=wsP['B1'].value\n",
        "  for i in range(lastRow):#loop through all rows to identify next run\n",
        "    if (i==0): i=1\n",
        "    if (int(lastRun)<i or lngResetRow==i): #Found a row for run\n",
        "      whoAmI=wsP['H'+str(i)].value\n",
        "      if (whoAmI==runnerName or lngResetRow==i):       \n",
        "        #read all run parameters and exit. else continue searching for the row\n",
        "        strModelName=wsP['A'+str(i)].value\n",
        "        strBaseModel=wsP['B'+str(i)].value\n",
        "        lngepochs=int(wsP['C'+str(i)].value)\n",
        "        lngTrainsize=int(wsP['D'+str(i)].value)\n",
        "        lngValSize=int(wsP['E'+str(i)].value)\n",
        "        lngTestSize=int(wsP['F'+str(i)].value)\n",
        "        lngImageSize=(wsP['G'+str(i)].value)\n",
        "        strBatchNorm=wsP['I'+str(i)].value\n",
        "        lngFrozenLayers=int(wsP['J'+str(i)].value)\n",
        "        strLossFunction=wsP['K'+str(i)].value\n",
        "        wsP['B1']=i\n",
        "        lngFound=i #setting to 1 to indicate model is set\n",
        "        break\n",
        "if (lngFound==0):#Not found, set with default values\n",
        "  strModelName=\"FCN8\"\n",
        "  strBaseModel=\"vgg16\"\n",
        "  strBaseModel=\"vgg16\"\n",
        "  lngepochs=50\n",
        "  lngTrainsize=300\n",
        "  lngValSize=50\n",
        "  lngTestSize=50\n",
        "  lngImageSize=1\n",
        "  strBatchNorm=\"No\"\n",
        "  lngFrozenLayers=5\n",
        "  strLossFunction=\"focal\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ce-wCcnqakDc",
        "colab_type": "code",
        "outputId": "01307cfc-03a8-429e-ca46-93dd94958563",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "  lngFound,strModelName,  strBaseModel,  lngepochs,  lngTrainsize,  lngValSize,  lngTestSize,lngImageSize"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 'Unet', None, 50, 300, 50, 20, 0.75)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QOb1_1rDfZ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class UNET:\n",
        "\n",
        "   class unet:\n",
        "        \"\"\" Unet architecture\n",
        "\n",
        "               Usage :\n",
        "                   unet = Semantic_Segmentation.models.UNET.unet(input_shape,no_classes,regularizer,summary)\n",
        "                   model = unet.build()\n",
        "\n",
        "               # Arguments\n",
        "                   input_shape : size of the input image ,in tuple.\n",
        "                   n_classes : the number of target class.\n",
        "                               dtype --> int\n",
        "                   regularizer : the regularizing value, it uses L2 regularizers on the kernel/filters.\n",
        "                               dtype --> float   default -->None\n",
        "                   summary : If True prints the model summary\n",
        "                                default --> True\n",
        "           \"\"\"\n",
        "        build = lambda self: self.__architecture()\n",
        "        weight_decay = lambda self, x: None if x == None else l2(x)\n",
        "        batchnorm = lambda self,x: BatchNormalization(beta_regularizer=l2(0.001),gamma_regularizer=l2(0.001)) if x else Activation('linear')\n",
        "\n",
        "        def __init__(self,input_shape,n_classes,regularizer=None,BatchNorm=True,summary=True):\n",
        "            self.input_shape = input_shape\n",
        "            self.n_classes = n_classes\n",
        "            self.regularizer = regularizer\n",
        "            self.BatchNorm = BatchNorm\n",
        "            self.summary = summary\n",
        "\n",
        "\n",
        "        def __architecture(self):\n",
        "\n",
        "            input = Input(self.input_shape)\n",
        "\n",
        "            conv1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal',kernel_regularizer=self.weight_decay(self.regularizer))(input)\n",
        "            conv1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal',kernel_regularizer=self.weight_decay(self.regularizer))(conv1)\n",
        "            batchnorm1 = self.batchnorm(self.BatchNorm)(conv1)\n",
        "            pool1 = MaxPooling2D(pool_size=(2, 2))(batchnorm1)\n",
        "             \n",
        "            \n",
        "            conv2 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal',kernel_regularizer=self.weight_decay(self.regularizer))(pool1)\n",
        "            conv2 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal',kernel_regularizer=self.weight_decay(self.regularizer))(conv2)\n",
        "            batchnorm2 = self.batchnorm(self.BatchNorm)(conv2)\n",
        "            pool2 = MaxPooling2D(pool_size=(2, 2))(batchnorm2)\n",
        "\n",
        "            \n",
        "            conv3 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal',kernel_regularizer=self.weight_decay(self.regularizer))(pool2)\n",
        "            conv3 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal',kernel_regularizer=self.weight_decay(self.regularizer))(conv3)\n",
        "            batchnorm3 = self.batchnorm(self.BatchNorm)(conv3)\n",
        "            pool3 = MaxPooling2D(pool_size=(2, 2))(batchnorm3)\n",
        "\n",
        "            \n",
        "            conv4 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal',kernel_regularizer=self.weight_decay(self.regularizer))(pool3)\n",
        "            conv4 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal',kernel_regularizer=self.weight_decay(self.regularizer))(conv4)\n",
        "            batchnorm4 = self.batchnorm(self.BatchNorm)(conv4)\n",
        "            pool4 = MaxPooling2D(pool_size=(2, 2))(batchnorm4)\n",
        "\n",
        "            conv5 = Conv2D(1024, 3, activation='relu', padding='same', kernel_initializer='he_normal',kernel_regularizer=self.weight_decay(self.regularizer))(pool4)\n",
        "            conv5 = Conv2D(1024, 3, activation='relu', padding='same', kernel_initializer='he_normal',kernel_regularizer=self.weight_decay(self.regularizer))(conv5)\n",
        "            batchnorm5 = self.batchnorm(self.BatchNorm)(conv5)\n",
        "            up5 = Conv2DTranspose(512, 4, strides=(2,2), padding='same', kernel_regularizer=self.weight_decay(self.regularizer), name='upsample_5')(batchnorm5)\n",
        "\n",
        "            merge6 = concatenate([batchnorm4, up5], axis=3)\n",
        "            conv6 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=self.weight_decay(self.regularizer))(merge6)\n",
        "            conv6 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=self.weight_decay(self.regularizer))(conv6)\n",
        "            batchnorm6 = self.batchnorm(self.BatchNorm)(conv6)\n",
        "            up6 = Conv2DTranspose(256, 4, strides=(2,2), padding='same', kernel_regularizer=self.weight_decay(self.regularizer), name='upsample_6')(batchnorm6)\n",
        "\n",
        "            merge7 = concatenate([batchnorm3, up6], axis=3)\n",
        "            conv7 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=self.weight_decay(self.regularizer))(merge7)\n",
        "            conv7 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal',kernel_regularizer=self.weight_decay(self.regularizer))(conv7)\n",
        "            batchnorm7 = self.batchnorm(self.BatchNorm)(conv7)\n",
        "            up7 = Conv2DTranspose(128, 4, strides=(2,2), padding='same', kernel_regularizer=self.weight_decay(self.regularizer), name='upsample_7')(batchnorm7)\n",
        "\n",
        "            merge8 = concatenate([batchnorm2, up7], axis=3)\n",
        "            conv8 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge8)\n",
        "            conv8 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv8)\n",
        "            batchnorm8 = self.batchnorm(self.BatchNorm)(conv8)\n",
        "            up8 = Conv2DTranspose(64, 4, strides=(2,2), padding='same', kernel_regularizer=self.weight_decay(self.regularizer), name='upsample_8')(batchnorm8)\n",
        "\n",
        "            merge9 = concatenate([batchnorm1, up8], axis=3)\n",
        "            conv9 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal',kernel_regularizer=self.weight_decay(self.regularizer))(merge9)\n",
        "            conv9 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal',kernel_regularizer=self.weight_decay(self.regularizer))(conv9)\n",
        "            conv9 = Conv2D(self.n_classes, 1, activation='relu', padding='same', kernel_initializer='he_normal',kernel_regularizer=self.weight_decay(self.regularizer))(conv9)\n",
        "            op = Activation('softmax',name='softmax')(conv9)\n",
        "\n",
        "            model = Model(input=input, output=op,name='Unet')\n",
        "            if self.summary: print(model.summary())\n",
        "            return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class FCN:\n",
        "    \"\"\" Available models --> fcn32, fcn16, fcn8\n",
        "\n",
        "        All the three avilable with vgg16,vgg19,resnet50\n",
        "    \"\"\"\n",
        "\n",
        "    class fcn32:\n",
        "\n",
        "        \"\"\" FCN32 architecture, it doesn't use any skip connections and upsamples the image by the scale of 32.\n",
        "\n",
        "            Usage :\n",
        "                FCN32 = Semantic_Segmentation.models.FCN.fcn32(input_shape,no_classes,base_model_name,pretrained_weights,regularizer,summary)\n",
        "                model = FCN32.build()\n",
        "\n",
        "            # Arguments\n",
        "                input_shape : size of the input image ,in tuple.\n",
        "                n_classes : the number of target class.\n",
        "                            dtype --> int\n",
        "                base_model : name of the pre-trained cnn model on top of which FCN is built.\n",
        "                            dtype --> string  default --> 'vgg16'\n",
        "                weight_path : path to the pre_trained weight files.\n",
        "                            dtype --> string  default --> 'imagenet'\n",
        "                regularizer : the regularizing value, it uses L2 regularizers on the kernel/filters.\n",
        "                            dtype --> float   default -->None\n",
        "                summary : If True prints the model summary\n",
        "                                default --> True\n",
        "        \"\"\"\n",
        "        weight_decay = lambda self,x: l2(x) if type(x)==int else None\n",
        "        build = lambda self: self.__decoder(self.encoder())\n",
        "\n",
        "        def __init__(self,input_shape,n_classes,base_model='vgg16',weight_path='imagenet',regularizer = None,summary=True):\n",
        "            self.base_model = base_model\n",
        "            self.input_shape = input_shape\n",
        "            self.n_classes = n_classes\n",
        "            self.weightpath = weight_path\n",
        "            self.regularizer = regularizer\n",
        "            self.summary = summary\n",
        "\n",
        "\n",
        "        def encoder(self):\n",
        "\n",
        "            \"\"\" Builds the encoder layer on top of the given pre-trained CNN model\n",
        "\n",
        "                # Returns\n",
        "                    model_en : the architecture of the encoder part\n",
        "            \"\"\"\n",
        "            if self.base_model == 'vgg16':\n",
        "                base = vgg16.VGG16(include_top=False, weights=self.weightpath, pooling=None, input_shape=self.input_shape)\n",
        "            elif self.base_model == 'vgg19':\n",
        "                base = vgg19.VGG19(include_top=False, weights=self.weightpath, pooling=None, input_shape=self.input_shape)\n",
        "            else: base = resnet50.ResNet50(include_top=False, weights=self.weightpath, pooling=None, input_shape=self.input_shape)\n",
        "\n",
        "            layer_encoder = Conv2D(4096, (7, 7), padding='same', activation='relu', kernel_regularizer=self.weight_decay(self.regularizer),\n",
        "                                   name='conv_en1')(base.output)\n",
        "            layer_encoder = Conv2D(4096, (1, 1), padding='same', activation='relu', kernel_regularizer=self.weight_decay(self.regularizer),\n",
        "                                   name='conv_en2')(layer_encoder)\n",
        "\n",
        "            encoder_model = Model(base.input,layer_encoder)\n",
        "            return encoder_model\n",
        "\n",
        "\n",
        "        def __decoder(self,model_en):\n",
        "\n",
        "            \"\"\" Builds the decoder layer on top of the encoder part.\n",
        "\n",
        "                # Arguments\n",
        "                    model_en : the model architecture of the encoder part\n",
        "\n",
        "                # Returns\n",
        "                    model : the full model architecture, both encoder and decoder combined\n",
        "            \"\"\"\n",
        "            layer_decoder = Conv2D(self.n_classes, (1,1), padding='same', activation='relu', kernel_regularizer=self.weight_decay(self.regularizer),\n",
        "                                   name = 'conv_dec')(model_en.output)\n",
        "            layer_decoder = Conv2DTranspose(self.n_classes, (64,64), strides=(32,32), padding = 'same', kernel_regularizer = self.weight_decay(self.regularizer),\n",
        "                                            name = 'deconv')(layer_decoder)\n",
        "            output = Activation('softmax',name='softmax')(layer_decoder)\n",
        "\n",
        "            model = Model(model_en.input,output,name='FCN32-'+self.base_model)\n",
        "            if self.summary: print(model.summary())\n",
        "            return model\n",
        "\n",
        "\n",
        "        def skip_connections(self,model_en):\n",
        "\n",
        "            \"\"\" Builds the skip connections based on the pre-trained CNN models for FCN16 and FCN8.\n",
        "                NOTE- FCN32 doesnt use this function, its a base for other class/FCN16,FCN8\n",
        "\n",
        "                # Arguments:\n",
        "                    model_en : the model architecture of the encoder part\n",
        "            \"\"\"\n",
        "            if self.base_model == 'resnet50': skip_1,skip_2 = model_en.get_layer(model_en.layers[112].name).output, model_en.get_layer(model_en.layers[50].name).output\n",
        "            elif self.base_model == 'vgg16': skip_1,skip_2 = model_en.get_layer(model_en.layers[14].name).output, model_en.get_layer(model_en.layers[10].name).output\n",
        "            else: skip_1,skip_2 = model_en.get_layer(model_en.layers[16].name).output, model_en.get_layer(model_en.layers[11].name).output\n",
        "            return skip_1,skip_2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    class fcn16(fcn32):\n",
        "\n",
        "        \"\"\" FCN16 architecture, it uses one skip connection and upsamples the image by the scale of 16.\n",
        "\n",
        "            Usage :\n",
        "                FCN16 = Semantic_Segmentation.models.FCN.fcn16(input_shape,no_classes,base_model_name,pretrained_weights,regularizer,summary)\n",
        "                model = FCN16.build()\n",
        "\n",
        "            # Arguments\n",
        "                input_shape : size of the input image ,in tuple.\n",
        "                n_classes : the number of target class.\n",
        "                            dtype --> int\n",
        "                base_model : name of the pre-trained cnn model on top of which FCN is built.\n",
        "                            dtype --> string  default --> 'vgg16'\n",
        "                weight_path : path to the pre_trained weight files.\n",
        "                            dtype --> string  default --> 'imagenet'\n",
        "                regularizer : the regularizing value, it uses L2 regularizers on the kernel/filters.\n",
        "                            dtype --> float   default -->None\n",
        "                summary : If True prints the model summary\n",
        "                                default --> True\n",
        "        \"\"\"\n",
        "\n",
        "        ## NOTE : It wraps the properties of FCN32, same arguments and encoder part but it uses diff decoder function\n",
        "        ##since the decoder includes skip connection.\n",
        "        build = lambda self: self.__decoder(super().encoder())\n",
        "\n",
        "        def __decoder(self,model_en):\n",
        "\n",
        "            skip,_ = super().skip_connections(model_en)\n",
        "\n",
        "            layer_decoder = Conv2DTranspose(skip.get_shape().as_list()[-1], (4, 4), strides=(2, 2), padding='same',\n",
        "                                            kernel_regularizer=super().weight_decay(self.regularizer), name='deconv1')(model_en.output)\n",
        "            layer_decoder = Add(name='skip1')([layer_decoder, skip])\n",
        "            layer_decoder = Conv2DTranspose(self.n_classes, (32,32), strides=(16, 16), padding='same',\n",
        "                                            kernel_regularizer=super().weight_decay(self.regularizer), name='deconv3')(layer_decoder)\n",
        "            output = Activation('softmax',name='softmax')(layer_decoder)\n",
        "\n",
        "            model = Model(model_en.input, output,name='FCN16-'+self.base_model)\n",
        "            if self.summary: print(model.summary())\n",
        "            return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    class fcn8(fcn32):\n",
        "\n",
        "        \"\"\" FCN8 architecture, it uses two skip connections and upsamples the image by the scale of 8, preserves most of the information.\n",
        "\n",
        "            Usage :\n",
        "                FCN8 = Semantic_Segmentation.models.FCN.fcn8(input_shape,no_classes,base_model_name,pretrained_weights,regularizer,summary)\n",
        "                model = FCN8.build()\n",
        "\n",
        "            # Arguments\n",
        "                input_shape : size of the input image ,in tuple.\n",
        "                n_classes : the number of target class.\n",
        "                            dtype --> int\n",
        "                base_model : name of the pre-trained cnn model on top of which FCN is built.\n",
        "                            dtype --> string  default --> 'vgg16'\n",
        "                weight_path : path to the pre_trained weight files.\n",
        "                            dtype --> string  default --> 'imagenet'\n",
        "                regularizer : the regularizing value, it uses L2 regularizers on the kernel/filters.\n",
        "                            dtype --> float   default -->None\n",
        "                summary : If True prints the model summary\n",
        "                                default --> True\n",
        "        \"\"\"\n",
        "\n",
        "        ## NOTE : It wraps the properties of FCN32, same arguments and encoder part but it uses diff decoder function\n",
        "        ##since the decoder includes 2 skip connections.\n",
        "\n",
        "        build = lambda self: self.__decoder(super().encoder())\n",
        "\n",
        "        def __decoder(self,model_en):\n",
        "\n",
        "            skip_1,skip_2 = super().skip_connections(model_en)\n",
        "\n",
        "            layer_decoder = Conv2DTranspose(skip_1.get_shape().as_list()[-1], (4, 4), strides=(2, 2), padding='same',\n",
        "                                            kernel_regularizer=super().weight_decay(self.regularizer), name='deconv1')(model_en.output)\n",
        "            layer_decoder = Add(name='skip1')([layer_decoder, skip_1])\n",
        "            layer_decoder = Conv2DTranspose(skip_2.get_shape().as_list()[-1], (4, 4), strides=(2, 2), padding='same',\n",
        "                                            kernel_regularizer=super().weight_decay(self.regularizer), name='deconv2')(layer_decoder)\n",
        "            layer_decoder = Add(name='skip2')([layer_decoder, skip_2])\n",
        "            layer_decoder = Conv2DTranspose(self.n_classes, (16, 16), strides=(8, 8), padding='same',\n",
        "                                            kernel_regularizer=super().weight_decay(self.regularizer), name='deconv3')(layer_decoder)\n",
        "            output = Activation('softmax',name='softmax')(layer_decoder)\n",
        "\n",
        "            model = Model(model_en.input,output,name='FCN8-'+self.base_model)\n",
        "            if self.summary: print(model.summary())\n",
        "            return model\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBw8SBhp4Ner",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class focal_loss:\n",
        "\n",
        "    \"\"\" A loss function similar to cross_entropy\n",
        "\n",
        "        # Usage\n",
        "            model.compile('sgd',loss=focal_loss.loss,.......)\n",
        "\n",
        "        # Arguments\n",
        "            class_weights : weights for each class to solve the class imbalance problem.\n",
        "                            dtype --> array   default --> None\n",
        "            pixel_weights : weights for each pixels in order to segment certain part of the image clearly.\n",
        "                            dtype --> array   default --> None\n",
        "    \"\"\"\n",
        "    c_weights = lambda self,x: 1 if x==None else x\n",
        "    p_weights = lambda self,x: 1 if x==None else x\n",
        "    clipping = lambda self,x: K.clip(x, K.epsilon(), 1.-K.epsilon())\n",
        "\n",
        "\n",
        "    def __init__(self,class_weights=None, pixel_weights=None, gamma=2):\n",
        "        self.class_weights = class_weights\n",
        "        self.gamma = gamma\n",
        "        self.pixel_weights = pixel_weights\n",
        "\n",
        "    def loss(self,y_true,y_pred):\n",
        "\n",
        "        \"\"\" executes the focal loss\n",
        "\n",
        "            # Arguments\n",
        "                y_true : true class values\n",
        "                y_pred : predicted class values from the model\n",
        "            # Returns\n",
        "                fl : mean focal loss for the given batch\n",
        "         \"\"\"\n",
        "        y_pred = self.clipping(y_pred)\n",
        "        fl = -(K.sum((self.c_weights(self.class_weights) * K.pow(1.-y_pred,self.gamma) * (y_true * K.log(y_pred))),axis=-1))\n",
        "        fl = K.sum((self.p_weights(self.pixel_weights) * fl),axis=(1,2))\n",
        "        fl = K.mean(fl, axis=0)\n",
        "        return fl/1000\n",
        "\n",
        "\n",
        "\n",
        "class cross_entropy(focal_loss):\n",
        "\n",
        "    \"\"\" Categorical cross_entropy\n",
        "        NOTE : for binary classification it uses softmax instead sigmoid\n",
        "\n",
        "        # Usage\n",
        "            model.compile('sgd',loss=cross_entropy.loss,.......)\n",
        "\n",
        "        # Arguments\n",
        "            class_weights : weights for each class to solve the class imbalance problem.\n",
        "                            dtype --> array   default --> None\n",
        "            pixel_weights : weights for each pixels in order to segment certain part of the image clearly.\n",
        "                            dtype --> array   default --> None\n",
        "    \"\"\"\n",
        "\n",
        "    ## NOTE - this class inherits the properties of focal_loss class\n",
        "    def loss(self,y_true,y_pred):\n",
        "\n",
        "        \"\"\" executes the categorical cross-entropy\n",
        "\n",
        "            # Arguments\n",
        "                y_true : true class values\n",
        "                y_pred : predicted class values from the model\n",
        "            # Returns\n",
        "                ce : mean cross-entropy for the given batch\n",
        "        \"\"\"\n",
        "        y_pred = super().clipping(y_pred)\n",
        "        ce = -(K.sum((super().c_weights(self.class_weights) * (y_true * K.log(y_pred))),axis=-1))\n",
        "        ce = K.sum((super().p_weights(self.pixel_weights) * ce),axis=(1,2))\n",
        "        ce = K.mean(ce,axis=0)\n",
        "        return ce/1000\n",
        "\n",
        "\n",
        "\n",
        "class dice_loss(focal_loss):\n",
        "\n",
        "    \"\"\" Its similar to IoU, dice_coeff = (2*A^B)/A U B  dice_loss= 1- dice_coeff\n",
        "        # Usage\n",
        "            model.compile('sgd',loss=dice_loss.loss,.......)\n",
        "\n",
        "        # Arguments\n",
        "            class_weights : weights for each class to solve the class imbalance problem.\n",
        "                            dtype --> array   default --> None\n",
        "            pixel_weights : weights for each pixels in order to segment certain part of the image clearly.\n",
        "                            dtype --> array   default --> None\n",
        "    \"\"\"\n",
        "\n",
        "    ## NOTE - this class inherits the properties of focal_loss class\n",
        "    def loss(self,y_true,y_pred):\n",
        "\n",
        "        \"\"\" executes the dice loss\n",
        "\n",
        "            # Arguments\n",
        "                y_true : true class values\n",
        "                y_pred : predicted class values from the model\n",
        "            # Returns\n",
        "                dl : dice loss for the given batch\n",
        "        \"\"\"\n",
        "        y_pred = super().clipping(y_pred)\n",
        "        intersection = K.sum((super().c_weights(self.class_weights) * y_true * y_pred),axis=-1)\n",
        "        intersection = K.sum((super().p_weights(self.pixel_weights) * intersection),axis=(0,1,2))\n",
        "        union = K.sum( (super().c_weights(self.class_weights)*((y_true*y_true) + (y_pred*y_pred)) ),axis=-1)\n",
        "        union = K.sum((super().p_weights(self.pixel_weights) * union),axis=(0,1,2))\n",
        "        dl = 1. - ((2*intersection)/union)\n",
        "        return dl\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNm77P5wfOdF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Metrics:\n",
        "\n",
        "    __weighted_method = lambda self,x,y,string,w: (K.sum(x)/K.sum(y)) if string=='inter' else (K.sum(w*x)/K.sum(w*y))\n",
        "    __avg_method = lambda self,x,y,string,w: K.mean(x/y) if string=='intra' else self.__weighted_method(x,y,string,w)\n",
        "    clipping = lambda self,x: K.clip(x, K.epsilon(), 1.-K.epsilon())\n",
        "\n",
        "\n",
        "    def __metrics_base(self,y_true,y_pred):\n",
        "\n",
        "        \"\"\" Base for all the metrics defined below \"\"\"\n",
        "        y_true,y_pred = K.flatten(tf.math.argmax(y_true,axis=-1)), K.flatten(tf.math.argmax(y_pred,axis=-1))\n",
        "        con_mat = K.cast(tf.math.confusion_matrix(y_true,y_pred), K.floatx())\n",
        "        correct = tf.linalg.diag_part(con_mat)\n",
        "        total = K.sum(con_mat, axis=-1)\n",
        "        return correct,total,con_mat\n",
        "\n",
        "\n",
        "    def accuracy(self,y_true,y_pred):\n",
        "\n",
        "        \"\"\" computes the accuracy\n",
        "\n",
        "            # Arguments\n",
        "                y_true : target value\n",
        "                y_pred : predicted class value\n",
        "            # Returns\n",
        "                acc : overall accuracy\n",
        "        \"\"\"\n",
        "        correct,total,_ = self.__metrics_base(y_true,y_pred)\n",
        "        return ( K.sum(correct) / K.sum(total) )\n",
        "\n",
        "\n",
        "    def IoU(self, y_true, y_pred,average='inter',weights=None):\n",
        "\n",
        "        \"\"\" Intersection over Union , IoU = A^B/(A U B - A^B)\n",
        "           Computes the percentage overlap with the target image.\n",
        "\n",
        "            # Arguments\n",
        "                y_true : target value\n",
        "                y_pred : predicted class value\n",
        "                average : 'inter' --> computes the IoU score overall  'intra' --> computes the score for each calss and computes the average\n",
        "                        'weighted' --> computes the weighted average , useful for imabalanced class.\n",
        "                weights :  only if average is specified 'weighted', weights for the respective classes.\n",
        "            # Returns\n",
        "                IoU score\n",
        "        \"\"\"\n",
        "        _, _, con_mat = self.__metrics_base(y_true, y_pred)\n",
        "        intersection = tf.linalg.diag_part(con_mat)\n",
        "        ground_truth_set = K.sum(con_mat, axis=1)\n",
        "        predicted_set = K.sum(con_mat, axis=0)\n",
        "        union = ground_truth_set + predicted_set - intersection\n",
        "        return self.__avg_method(intersection,union,average,weights)\n",
        "\n",
        "\n",
        "    def recall(self,y_true,y_pred,average='inter',weights=None):\n",
        "\n",
        "        \"\"\" Computes the recall score over each given class and gives the overall score.  recall = TP/TP+FN\n",
        "\n",
        "            # Arguments\n",
        "                y_true : target value\n",
        "                y_pred : predicted class value\n",
        "                average : 'inter' --> computes the recall score overall  'intra' --> computes the score for each calss and computes the average\n",
        "                        'weighted' --> computes the weighted average , useful for imabalanced class.\n",
        "                weights :  only if average is specified 'weighted', weights for the respective classes.\n",
        "            # Returns\n",
        "                recall score\n",
        "        \"\"\"\n",
        "        correct,total,_ = self.__metrics_base(y_true,y_pred)\n",
        "        return self.__avg_method(correct,total,average,weights)\n",
        "\n",
        "\n",
        "    def precision(self,y_true,y_pred,average='inter',weights=None):\n",
        "\n",
        "        \"\"\" Computes the precision over each given class and returns the overall score.  precision = TP/TP+FP\n",
        "\n",
        "            # Arguments\n",
        "                y_true : target value\n",
        "                y_pred : predicted class value\n",
        "                average : 'inter' --> computes the precision score overall  'intra' --> computes the score for each calss and computes the average\n",
        "                        'weighted' --> computes the weighted average , useful for imabalanced class.\n",
        "                weights :  only if average is specified 'weighted', weights for the respective classes.\n",
        "            # Returns\n",
        "                precision score\n",
        "        \"\"\"\n",
        "        correct,_,con_mat = self.__metrics_base(y_true,y_pred)\n",
        "        total = K.sum(con_mat,axis=0)\n",
        "        return self.__avg_method(correct,total,average,weights)\n",
        "\n",
        "\n",
        "    def f1score(self,y_true,y_pred,average='inter',weights=None):\n",
        "\n",
        "        \"\"\" Computes the f1 score over each given class and returns the overall score.  f1 = (2*precision*recall)/(precision+recall)\n",
        "\n",
        "            # Arguments\n",
        "                y_true : target value\n",
        "                y_pred : predicted class value\n",
        "                average : 'inter' --> computes the f1 score overall  'intra' --> computes the score for each calss and computes the average\n",
        "                            'weighted' --> computes the weighted average , useful for imabalanced class.\n",
        "                weights :  only if average is specified 'weighted', weights for the respective classes.\n",
        "            # Returns\n",
        "                 f1 score\n",
        "        \"\"\"\n",
        "        precision = self.precision(y_true,y_pred,average,weights)\n",
        "        recall = self.recall(y_true,y_pred,average,weights)\n",
        "        return ((2*precision*recall)/(precision+recall))\n",
        "\n",
        "\n",
        "\n",
        "    def dice_coeffiecient(self,y_true,y_pred,average='inter',weights=None):\n",
        "        \"\"\" Computes the dice score over each given class and returns the overall score.\n",
        "\n",
        "                # Arguments\n",
        "                    y_true : target value\n",
        "                    y_pred : predicted class value\n",
        "                    average : 'inter' --> computes the dice score overall  'intra' --> computes the score for each calss and computes the average\n",
        "                                    'weighted' --> computes the weighted average , useful for imabalanced class.\n",
        "                    weights :  only if average is specified 'weighted', weights for the respective classes.\n",
        "                # Returns\n",
        "                    dice score\n",
        "                \"\"\"\n",
        "\n",
        "        y_pred = self.clipping(y_pred)\n",
        "        intersection = 2 * K.sum((y_true * y_pred),axis=(0,1,2))\n",
        "        union = K.sum( (y_true*y_true) + (y_pred*y_pred),axis=(0,1,2))\n",
        "        return self.__avg_method(intersection,union,average,weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nTwd5v1gQba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_convert(data1,data2,data3,data4):\n",
        "  #for i in tqdm(range(data1.shape[0])):\n",
        "  #    for j in range(data1.shape[1]):\n",
        "  #      for k in range(data1.shape[2]):\n",
        "  #        for c in range(12):\n",
        "  #          if max(abs(data1[i,j,k]-data2[c])) == 0: \n",
        "  #            data4[i,j,k] = data3[c]\n",
        "  #            break\n",
        "  #return data4  \n",
        "#above changed to below for faster execution\n",
        "  for i in tqdm(range(n_classes)): \n",
        "    data4[(np.where((np.sum(abs(data1-data2[i]),axis=-1)) == 0))] = data3[i]\n",
        "  return data4\n",
        "      \n",
        "def freeze_layers(n_layers):\n",
        "    for i in range(n_layers):\n",
        "      model.layers[i].trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtcVwi0UNmOF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## color map instruction\n",
        "label_value = np.array([[0,0,0],[70,70,70],[153,153,190],[160, 170, 250],[153, 153, 153],[50, 234, 157],[128, 64, 128],[232, 35, 244],[35, 142, 107],[142, 0, 0],[156, 102, 102],[0, 220, 220]])\n",
        "label_class = np.array(['Unlabeled','Building','Fence','Other','Pole','Road line','Road','Sidewalk','Vegetation','Car','Wall','Traffic sign'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0GwHwp-hK-v",
        "colab_type": "code",
        "outputId": "449e9a4f-54f2-45ec-8638-4f36af81ccbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "#List the files\n",
        "image_list = os.listdir(path_x)\n",
        "mask_list = os.listdir(path_y)\n",
        "temp_image_list=image_list#Creating temp variable so to align the same file names\n",
        "#add   image path\n",
        "image_list = [path_x+i for i in image_list]\n",
        "mask_list = [path_y+i for i in temp_image_list]#using temp_image_list, otherwise file names not same as in RGB and SEG\n",
        "\n",
        "#Load image and resize\n",
        "def loadImagesCV2(image_list,xsizepercent,ysizepercent):\n",
        "  images = []\n",
        "  for n in  tqdm(range(len(image_list))):\n",
        "      img = cv2.imread(image_list[n],cv2.IMREAD_COLOR)\n",
        "      res=cv2.resize(img, (0, 0), fx = xsizepercent, fy =ysizepercent)\n",
        "      images.append(res)\n",
        "  return images\n",
        "\n",
        "## reading the data from image\n",
        "data_x = np.array(loadImagesCV2(image_list,lngImageSize,lngImageSize))\n",
        "data_y =np.array(loadImagesCV2(mask_list,lngImageSize,lngImageSize))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 479/479 [00:05<00:00, 84.19it/s]\n",
            "100%|██████████| 479/479 [00:03<00:00, 159.30it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFIhNJdQty_w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LhBaRE5foDG",
        "colab_type": "code",
        "outputId": "ae67a46d-56fd-44d2-bf92-3a8f8a500b67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data_y.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(479, 384, 384, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLmPBAiABnwb",
        "colab_type": "code",
        "outputId": "cd1347ef-75a8-479d-9c89-de10155d6b17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X=data_x\n",
        "Y=data_y\n",
        "data_x,data_y = data_x[0:400],data_y[0:400]\n",
        "data_y.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400, 384, 384, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alJ8BTi4hjkC",
        "colab_type": "code",
        "outputId": "a0cf2dc1-7ee1-4f86-9cc2-df79fbe5dcad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## getting the data(target) ready as one-hot encoded before passing to the model\n",
        "data_y_1hot = np.zeros([data_y.shape[0],data_y.shape[1],data_y.shape[2],n_classes])\n",
        "label_1hot = tf.one_hot([0,1,2,3,4,5,6,7,8,9,10,11],depth = n_classes)\n",
        "label_1hot = K.eval(label_1hot)\n",
        "\n",
        "data_y_1hot = data_convert(data_y,label_value,label_1hot,data_y_1hot)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:24<00:00,  2.01s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFCyrdV4hkOC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## after this build a model using the above functions split the data into train and test and run it using different loss functions and metrics which are available above\n",
        "## data_x --> for input features,  data_y_1hot --> for target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCPIZRI5gQlJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_x,train_y = data_x[0:lngTrainsize],data_y_1hot[0:lngTrainsize]\n",
        "val_x,val_y = data_x[lngTrainsize:lngTrainsize+lngValSize],data_y_1hot[lngTrainsize:lngTrainsize+lngValSize]\n",
        "test_x,test_y = data_x[lngTrainsize+lngValSize:lngTrainsize+lngValSize+lngTestSize],data_y_1hot[lngTrainsize+lngValSize:lngTrainsize+lngValSize+lngTestSize]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQecSPNHZDnR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_x,val_x = train_x/255 , val_x/255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lC_NjgpkJVb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get batchnormalization\n",
        "if (strBatchNorm==\"Yes\"):\n",
        "  batchNorm=True\n",
        "else:\n",
        "  batchNorm=False\n",
        "\n",
        "\n",
        "if strModelName==\"Unet\":\n",
        "  fcn = UNET.unet((data_x.shape[1],data_x.shape[2],3),n_classes,0.001,batchNorm)\n",
        "if (strModelName==\"FCN32\"):\n",
        "  fcn = FCN.fcn32((data_x.shape[1],data_x.shape[2],3),n_classes,strBaseModel,\"imagenet\",0.001)\n",
        "if (strModelName==\"FCN16\"):\n",
        "  fcn = FCN.fcn16((data_x.shape[1],data_x.shape[2],3),n_classes,strBaseModel,\"imagenet\",0.001)\n",
        "if (strModelName==\"FCN8\"):\n",
        "  fcn = FCN.fcn8((data_x.shape[1],data_x.shape[2],3),n_classes,  strBaseModel,\"imagenet\",0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIkrdYyjMcTn",
        "colab_type": "code",
        "outputId": "133d354a-ce8c-4f97-c476-fbc5341d914b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "(data_x.shape[1],data_x.shape[2],3),n_classes"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((384, 384, 3), 12)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHs1pA4Ukcfm",
        "colab_type": "code",
        "outputId": "f667236e-3220-4982-93e6-7f34d3b3dc97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = fcn.build()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"Unet\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 512, 512, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 512, 512, 64) 1792        input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 512, 512, 64) 36928       conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 512, 512, 64) 256         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, 256, 256, 64) 0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 256, 256, 128 73856       max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 256, 256, 128 147584      conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 256, 256, 128 512         conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2D)  (None, 128, 128, 128 0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 128, 128, 256 295168      max_pooling2d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 128, 128, 256 590080      conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 128, 128, 256 1024        conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2D)  (None, 64, 64, 256)  0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 64, 64, 512)  1180160     max_pooling2d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 64, 64, 512)  2359808     conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 64, 64, 512)  2048        conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2D)  (None, 32, 32, 512)  0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 32, 32, 1024) 4719616     max_pooling2d_8[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 32, 32, 1024) 9438208     conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 32, 32, 1024) 4096        conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "upsample_5 (Conv2DTranspose)    (None, 64, 64, 512)  8389120     batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 64, 64, 1024) 0           batch_normalization_12[0][0]     \n",
            "                                                                 upsample_5[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 64, 64, 512)  4719104     concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 64, 64, 512)  2359808     conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 64, 64, 512)  2048        conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "upsample_6 (Conv2DTranspose)    (None, 128, 128, 256 2097408     batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 128, 128, 512 0           batch_normalization_11[0][0]     \n",
            "                                                                 upsample_6[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 128, 128, 256 1179904     concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 128, 128, 256 590080      conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 128, 128, 256 1024        conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "upsample_7 (Conv2DTranspose)    (None, 256, 256, 128 524416      batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 256, 256, 256 0           batch_normalization_10[0][0]     \n",
            "                                                                 upsample_7[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 256, 256, 128 295040      concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 256, 256, 128 147584      conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 256, 256, 128 512         conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "upsample_8 (Conv2DTranspose)    (None, 512, 512, 64) 131136      batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 512, 512, 128 0           batch_normalization_9[0][0]      \n",
            "                                                                 upsample_8[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 512, 512, 64) 73792       concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 512, 512, 64) 36928       conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 512, 512, 12) 780         conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "softmax (Activation)            (None, 512, 512, 12) 0           conv2d_38[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 39,399,820\n",
            "Trainable params: 39,394,060\n",
            "Non-trainable params: 5,760\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:87: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"Unet\", inputs=Tensor(\"in..., outputs=Tensor(\"so...)`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fy631zUiX6VR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggGBGU1kXC39",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt =  keras.optimizers.RMSprop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBPt6d8Ue0BT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XX4D4D-gkflm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "freeze_layers(lngFrozenLayers)  ## here 5 is the no of layers needed to be freezed\n",
        "#get loss function\n",
        "if (strLossFunction==\"focal\"):\n",
        "  fnLoss=focal_loss().loss\n",
        "if (strLossFunction==\"cross entropy\"):\n",
        "  fnLoss=cross_entroy()\n",
        "if (strLossFunction==\"dice\"):\n",
        "  fnLoss=dice_loss().loss\n",
        "model.compile(opt,loss = fnLoss,metrics=[Metrics().accuracy,Metrics().IoU])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXjblzzZk9ud",
        "colab_type": "code",
        "outputId": "a283bbc5-3be7-48b3-f7cd-cae5c137a0a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        }
      },
      "source": [
        "epochs=  lngepochs\n",
        "batchsize=8\n",
        "\n",
        "# simple early stopping\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=1000)\n",
        "history=model.fit(train_x,train_y,batchsize,epochs, validation_data=(val_x,val_y),callbacks=[es])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 300 samples, validate on 50 samples\n",
            "Epoch 1/50\n",
            "300/300 [==============================] - 36s 120ms/step - loss: 1541.0254 - accuracy: 0.3366 - IoU: 0.2030 - val_loss: 1645.4482 - val_accuracy: 0.3169 - val_IoU: 0.1888\n",
            "Epoch 2/50\n",
            "300/300 [==============================] - 26s 88ms/step - loss: 758.4328 - accuracy: 0.3468 - IoU: 0.2117 - val_loss: 153.9011 - val_accuracy: 0.5869 - val_IoU: 0.4189\n",
            "Epoch 3/50\n",
            "300/300 [==============================] - 26s 88ms/step - loss: 137.1530 - accuracy: 0.5171 - IoU: 0.3599 - val_loss: 121.4088 - val_accuracy: 0.4912 - val_IoU: 0.3272\n",
            "Epoch 4/50\n",
            "300/300 [==============================] - 26s 88ms/step - loss: 152.5690 - accuracy: 0.6460 - IoU: 0.4961 - val_loss: 98.4451 - val_accuracy: 0.7223 - val_IoU: 0.5703\n",
            "Epoch 5/50\n",
            "300/300 [==============================] - 26s 88ms/step - loss: 102.8369 - accuracy: 0.7260 - IoU: 0.5823 - val_loss: 95.4578 - val_accuracy: 0.7344 - val_IoU: 0.5845\n",
            "Epoch 6/50\n",
            "300/300 [==============================] - 26s 88ms/step - loss: 84.4565 - accuracy: 0.7093 - IoU: 0.5700 - val_loss: 82.9167 - val_accuracy: 0.7545 - val_IoU: 0.6105\n",
            "Epoch 7/50\n",
            "300/300 [==============================] - 26s 88ms/step - loss: 126.3208 - accuracy: 0.7502 - IoU: 0.6156 - val_loss: 79.1422 - val_accuracy: 0.7390 - val_IoU: 0.5912\n",
            "Epoch 8/50\n",
            "300/300 [==============================] - 26s 88ms/step - loss: 60.0062 - accuracy: 0.8034 - IoU: 0.6747 - val_loss: 86.3265 - val_accuracy: 0.7339 - val_IoU: 0.5854\n",
            "Epoch 9/50\n",
            "300/300 [==============================] - 26s 88ms/step - loss: 60.2046 - accuracy: 0.7792 - IoU: 0.6474 - val_loss: 70.1340 - val_accuracy: 0.7742 - val_IoU: 0.6350\n",
            "Epoch 10/50\n",
            "300/300 [==============================] - 26s 88ms/step - loss: 89.0274 - accuracy: 0.7754 - IoU: 0.6477 - val_loss: 64.8983 - val_accuracy: 0.7690 - val_IoU: 0.6289\n",
            "Epoch 11/50\n",
            "300/300 [==============================] - 26s 88ms/step - loss: 48.8983 - accuracy: 0.8217 - IoU: 0.7011 - val_loss: 71.0305 - val_accuracy: 0.7533 - val_IoU: 0.6075\n",
            "Epoch 12/50\n",
            "300/300 [==============================] - 26s 88ms/step - loss: 130.8576 - accuracy: 0.7667 - IoU: 0.6369 - val_loss: 72.7519 - val_accuracy: 0.7403 - val_IoU: 0.5911\n",
            "Epoch 13/50\n",
            "300/300 [==============================] - 26s 88ms/step - loss: 47.1739 - accuracy: 0.8185 - IoU: 0.6964 - val_loss: 68.6193 - val_accuracy: 0.7697 - val_IoU: 0.6304\n",
            "Epoch 14/50\n",
            "300/300 [==============================] - 26s 88ms/step - loss: 46.9480 - accuracy: 0.8193 - IoU: 0.6967 - val_loss: 74.2629 - val_accuracy: 0.7488 - val_IoU: 0.6004\n",
            "Epoch 15/50\n",
            "300/300 [==============================] - 26s 88ms/step - loss: 44.3128 - accuracy: 0.8312 - IoU: 0.7135 - val_loss: 63.3725 - val_accuracy: 0.7603 - val_IoU: 0.6153\n",
            "Epoch 16/50\n",
            "300/300 [==============================] - 26s 88ms/step - loss: 45.3808 - accuracy: 0.8211 - IoU: 0.7007 - val_loss: 75.2648 - val_accuracy: 0.7561 - val_IoU: 0.6097\n",
            "Epoch 17/50\n",
            "300/300 [==============================] - 26s 87ms/step - loss: 41.5575 - accuracy: 0.8370 - IoU: 0.7213 - val_loss: 71.5164 - val_accuracy: 0.7683 - val_IoU: 0.6251\n",
            "Epoch 18/50\n",
            "300/300 [==============================] - 26s 87ms/step - loss: 42.2825 - accuracy: 0.8281 - IoU: 0.7117 - val_loss: 60.1492 - val_accuracy: 0.7939 - val_IoU: 0.6599\n",
            "Epoch 19/50\n",
            "300/300 [==============================] - 26s 88ms/step - loss: 63.3873 - accuracy: 0.8254 - IoU: 0.7090 - val_loss: 64.5453 - val_accuracy: 0.7723 - val_IoU: 0.6317\n",
            "Epoch 20/50\n",
            "176/300 [================>.............] - ETA: 10s - loss: 44.8880 - accuracy: 0.8238 - IoU: 0.7059"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5_gbwzHyK3D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfoIG9out178",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# summarize history for accuracy\n",
        "plt.rcParams[\"figure.figsize\"] = (2,2)\n",
        "plt.plot(history.history['accuracy'])\n",
        "_=plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.savefig(logPath+\"myplot1.png\")\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.savefig(logPath+\"myplot2.png\")\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['IoU'])\n",
        "plt.plot(history.history['val_IoU'])\n",
        "plt.title('Model IOU')\n",
        "plt.ylabel('IoU')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.savefig(logPath+\"myplot3.png\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQ7s1wvUv3mm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred = model.predict(test_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZt4_f3Mr3pf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred[pred > 0.5] = 1\n",
        "pred[pred<0.5] = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmVB4gWHpW1H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict = np.zeros((test_x.shape[0],data_x.shape[1],data_x.shape[2],3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMfYY1EEoLRZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict = data_convert(pred,label_1hot,label_value,predict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWoJ14brpmP0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05yH4tgWqaE7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "#cv2_imshow(data_y[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xHRC53gqfY8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#cv2_imshow(data_y[8])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnW_eirMAlXK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def concat_images(imga, imgb):\n",
        "    \"\"\"\n",
        "    Combines two color image ndarrays side-by-side.\n",
        "    \"\"\"\n",
        "    ha,wa = imga.shape[:2]\n",
        "    hb,wb = imgb.shape[:2]\n",
        "    max_height = (np.max([ha, hb]))\n",
        "    total_width = (wa+wb)\n",
        "    new_img = np.zeros(shape=(max_height, total_width, 3))\n",
        "    new_img[:ha,:wa]=imga\n",
        "    new_img[:hb,wa:wa+wb]=imgb\n",
        "    return new_img\n",
        " \n",
        "output = concat_images(data_y[lngTrainsize+lngValSize+16],predict[16])\n",
        "\n",
        "#plt.imshow(output)\n",
        "cv2_imshow(output)\n",
        "\n",
        "#plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbKrjruivpm9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert to Unix timestamp\n",
        "d1_ts = time.mktime(stTime.timetuple())\n",
        "d2_ts = time.mktime(datetime.now().timetuple())\n",
        "\n",
        "# They are now in seconds, subtract and then divide by 60 to get minutes.\n",
        "minute= int(d2_ts-d1_ts) / 60"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgqGEhJYsYzI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainable_count = count_params(model.trainable_weights)\n",
        "non_trainable_count = count_params(model.non_trainable_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7H4wCb0gNlNu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Update Excel Report\n",
        "if (path.exists(logPath+\"Logfile.xlsx\")==True): #File exist, open it\n",
        "  wb = openpyxl.load_workbook(logPath+\"Logfile.xlsx\")\n",
        "  ws = wb.active\n",
        "else: #file not available, create it\n",
        "  wb = Workbook()\n",
        "  ws = wb.active\n",
        "  ws['A1']=\"Run Date\"\n",
        "  ws['B1']=\"Model\"\n",
        "  ws['C1']=\"Image Size\"\n",
        "  ws['D1']=\"Epocs\"\n",
        "  ws['E1']=\"Train Size\"\n",
        "  ws['F1']=\"Train Accuracy\"\n",
        "  ws['G1']=\"Train Loss\"\n",
        "  ws['H1']=\"Test Size\"\n",
        "  ws['I1']=\"Test Accuracy\"\n",
        "  ws['J1']=\"Test Loss\"\n",
        "  ws['K1']=\"IOU\"\n",
        "  ws['L1']=\"Time taken (min)\"\n",
        "  ws['M1']=\"Trainable Params\"\n",
        "  ws['N1']=\"Non-Trainable Params\"\n",
        "  ws['O1']=\"Batch size\"\n",
        "  ws['P1']=\"Optimizer\"\n",
        "  ws['Q1']=\"Optimizer Params\"\n",
        "  ws['R1']=\"Loss Function\"\n",
        "  ws['S1']=\"Comments\"\n",
        "  ws['T1']=\"EarlyStopping Epoch\"\n",
        "  ws['U1']=\"BatchNorm\"\n",
        "  ws['V1']=\"FrozenLayers\"\n",
        "colummn=ws['A']\n",
        "lastRow=len(colummn)+1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yia6hZrezlGD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ws['B'+str(lastRow)]=model.name #\"Unet\" #Model\n",
        "ws['C'+str(lastRow)]=str(data_x.shape[1])+str('x')+str(data_x.shape[2]) #Image Size\n",
        "ws['E'+str(lastRow)]=train_x.shape[0] #\"Train Size\"\n",
        "ws['H'+str(lastRow)]=val_x.shape[0] #\"Test Size\" \n",
        "ws['D'+str(lastRow)]=epochs #\"Epocs\"\n",
        "ws['O'+str(lastRow)]=batchsize #\"Batch size\"\n",
        "ws['R'+str(lastRow)]=str(fnLoss) #\"focal_loss\" #\"Loss Function\n",
        "ws['F'+str(lastRow)]=history.history['accuracy'][-1] #\"Train Accuracy\"\n",
        "ws['G'+str(lastRow)]=history.history['loss'][-1] #\"Train Loss\" \n",
        "ws['I'+str(lastRow)]=history.history['val_accuracy'][-1] #\"Test Accuracy\"\n",
        "ws['J'+str(lastRow)]=history.history['val_loss'][-1] #\"Test Loss\"\n",
        "ws['K'+str(lastRow)]=\"IOU\"\n",
        "ws['T'+str(lastRow)]=str(len(history.history['accuracy']))\n",
        "ws['A'+str(lastRow)] = dt_string #Run Date\n",
        "ws['L'+str(lastRow)] = minute #Time Take\n",
        "\n",
        "ws['M'+str(lastRow)] = trainable_count #Trainable Params\n",
        "ws['N'+str(lastRow)] = non_trainable_count #Non-Trainable Params\n",
        "ws['P'+str(lastRow)] =str(model.optimizer) #Optimizer\n",
        "ws['Q'+str(lastRow)] =str(model.optimizer.get_config()) #Optimizer Params\n",
        "ws['S'+str(lastRow)] =\"With Convert function as new and with early stopping\" #comments\n",
        "\n",
        "ws['U'+str(lastRow)] = strBatchNorm\n",
        "ws['V'+str(lastRow)] = lngFrozenLayers\n",
        "\n",
        "img1 = openpyxl.drawing.image.Image(logPath+\"myplot1.png\")\n",
        "img2 = openpyxl.drawing.image.Image(logPath+\"myplot2.png\")\n",
        "img3 = openpyxl.drawing.image.Image(logPath+\"myplot3.png\")\n",
        "ws.add_image(img1 ,'X'+str(lastRow))\n",
        "ws.add_image(img2,'AC'+str(lastRow))\n",
        "ws.add_image(img3,'AH'+str(lastRow))\n",
        "\n",
        "output=cv2.resize(output, (0, 0), fx = 0.25, fy =0.25)\n",
        "cv2.imwrite(logPath+'output.png',output)\n",
        "img1 = openpyxl.drawing.image.Image(logPath+\"output.png\")\n",
        "ws.add_image(img1,'AL'+str(lastRow))\n",
        "\n",
        "ws.row_dimensions[lastRow].height = 120\n",
        "wb.save(logPath+\"Logfile.xlsx\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLg9Z_LM1GmV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wbP.save(logPath+\"ModelParameters.xlsx\")  #run parameter marker, Saving at the end as if program crashes, it can run the same again"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJNnKnNW_7T-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "exit() #added this so that each time colab keeps some memory, reruning would crash.  using this now memory is reset back"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUgIObQHS4yy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
